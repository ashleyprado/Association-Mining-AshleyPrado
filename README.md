### Interactive Supermarket Simulation with Association Rule Mining

#### Author Information

- **Name**: Ashley Prado
- **Student ID**: 6409687
- **Course**: CAI 4002 - Artificial Intelligence
- **Semester**: Fall 2025

#### System Overview

This project is an interactive supermarket simulation that allows users to create transactions manually or import them from a CSV file. After loading the dataset, the system performs preprocessing to clean the data and then applies the Apriori and Eclat algorithms to generate association rules. A Streamlit UI presents these results in a simple, intuitive format, including product recommendations and confidence levels.

#### Technical Stack

- **Language**: Python 3.x
- **Key Libraries**:
  - `streamlit`
  - `pandas`
  - `numpy`
  - `time`
- **UI Framework**: Streamlit

#### Installation

##### Prerequisites

- [e.g., Python 3.8+, Node.js 14+, Java 11+]
- [Other requirements]

##### Setup

```bash
# Clone or extract project
### Prerequisites
- Python 3.8 or higher
- pip installed
- Ability to run Streamlit apps

# Install dependencies
pip install -r requirements.txt

# Run application
streamlit run src/app.py
```

#### Usage

##### 1. Load Data

- **Manual Entry**: Click items to create transactions
- **Import CSV**: Use "Import" button to load `sample_transactions.csv`
  -The app displays the number of loaded transactions.

##### 2. Preprocess Data

-Click Run Preprocessing.

The app cleans:
-Empty transactions
-Single-item transactions
-Duplicate entries
-Items with case inconsistencies
-tems with whitespace
-Invalid products
-A preprocessing report appears summarizing all issues fixed.

##### 3. Run Mining

-Select minimum support (default: 0.2).
-Select minimum confidence (default: 0.5).
-Click Run Apriori and Eclat.
-The app displays:
-Frequent itemsets
-Association rules
-Execution time for Apriori and Eclat
-Number of rules generated
-A comparison table

##### 4. Query Results

-Select a product from the dropdown (e.g., "milk").
-Choose which algorithm’s rules to use.
-The system shows:
-Products frequently bought with the selected item
-Confidence percentage
-A graphical bar representing strength
-A business recommendation

#### Algorithm Implementation

##### Apriori

The Apriori algorithm is implemented using a horizontal transaction representation. It generates candidate itemsets level-by-level and prunes itemsets that do not satisfy the minimum support requirement. Association rules are then generated by evaluating all subsets of each frequent itemset.

-Data structure: dictionary of {frozenset(itemset): support}

-Candidate generation: breadth-first, level-wise joining

-runing strategy: minimum support threshold

##### Eclat

The Eclat algorithm is implemented using a vertical representation, mapping each item to a TID-set (set of transaction IDs where the item appears). New itemsets are created through intersections of TID-sets, and depth-first search is used to explore itemset combinations.

-Data structure: {frozenset(itemset): set(TIDs)}

-Search strategy: depth-first recursion

-Intersection method: Python set intersection

##### CLOSET

CLOSET is not implemented in this projec

#### Performance Results

Tested on provided dataset (80-100 transactions after cleaning):

| Algorithm | Runtime (ms) | Rules Generated | Memory Usage |
| --------- | ------------ | --------------- | ------------ |
| Apriori   | 0.2400       | 19              | N/A          |
| Eclat     | 0.2200       | 19              | N/A          |

**Parameters**: min_support = 0.2, min_confidence = 0.5

**Analysis**: Both algorithms produced the same number of rules. Eclat executed slightly faster (0.22 ms) than Apriori (0.24 ms), which is consistent with expectations because Eclat relies on efficient TID-set intersections while Apriori generates larger sets of candidates. Differences are small due to the dataset size.

#### Project Structure

```
project-root/
├── src/
│   ├── algorithms/
│   │   ├── apriori.py
│   │   ├── eclat.py
│   │   └── __init__.py
│   ├── preprocessing/
│   │   ├── cleaner.py
│   │   └── __init__.py
│   └── app.py
├── data/
│   ├── sample_transactions.csv
│   └── products.csv
├── requirements.txt
├── README.md
└── REPORT.pdf
```

#### Data Preprocessing

Issues handled:

The following issues were detected and corrected during preprocessing:

- Empty transactions: 5 removed
- Single-item transactions: 6 removed
- Duplicate items: 9 instances cleaned
- Case inconsistencies: standardized to lowercase
- Invalid items: 2 removed
- Extra whitespace: trimmed from all item names

Before Cleaning:

- Total transactions: 100

After Cleaning:

- Valid transactions: 89
- Total items: 276
- Unique products: 30

#### Testing

Verified functionality:

- [✓] CSV import and parsing
- [✓] All preprocessing operations
- [✓] Three algorithm implementations
- [✓] Interactive query system
- [✓] Performance measurement

Test cases:

Dirty Data Test: Verified removal of invalid products, duplicates, and empty transactions.

Support/Confidence Test: Confirmed rule outputs change as thresholds change.

Recommendation Test: Queried common items and verified rules match expected associations.

#### Known Limitations

- Memory usage is not tracked.

- priori may slow down with very large datasets.

- Visualizations are intentionally simple for readability.

#### AI Tool Usage

ChatGPT was used to help structure the application flow, clarify Apriori and Eclat algorithm logic, draft preprocessing functions, and generate portions of the Streamlit UI. It also assisted in writing and formatting this README. All AI-generated components were reviewed, modified, and tested to ensure correctness and understanding.

#### References

- CAI 4002 Course Materials
- Agrawal & Srikant (1994) — Fast Algorithms for Mining Association Rules
- Zaki (2000) — Scalable Algorithms for Association Mining
- Streamlit Documentation — https://docs.streamlit.io
- pandas Documentation — https://pandas.pydata.org
